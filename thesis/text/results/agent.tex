\section{Wykorzystany agent - implementacja}

Implementacja agenta, który posłużył do wykonania eksperymentów opisywanych w tym rozdziale znajduje się w pliku \textit{actions\_estimator\_tf.py}. Agent w zależności od ustawień może realizować zarówno algorytmy \textit{kopiowania zachowań} jak i \textit{agregacji zbioru danych}. Implementacja agenta opiera się na głębokiej sieci konwolucyjnej modelującą funkcję wyboru $stan \rightarrow akcja$.


\subsection{Sieć neuronowa}

\subsubsection{Architektura}
Opis architektury znajduje się w rozdziale \ref{agent_net}.

\subsubsection{Pamięć powtórek}
Dla części scenariuszy (\textit{Obrona środka}) problemem przy uczeniu jest niezbalansowanie zbioru danych (niektóre akcje występują znacznie rzadziej niż inne). Zamiast popularnej metody \textit{oversamplingu} zastosowano rozwiązanie, w którym dla każdej z akcji przetrzymywana jest osobna pamięć powtórek. Przy wielkości batcha n i liczbie dostępnych akcji a z każdej z kolejnych pamięci powtórek pobierana jest losowa próbka o wielkości n/a. Uzyskane podpróbki łączone są w losowej kolejności i zwracane jako ostateczna próbka. Dzięki zastosowaniu takiego rozwiązania próbki danych uczących zawierają dla mniej wykonywanych akcji bardziej zróżnicowane rekordy, zamiast kilku powtórzeń tego samego doświadczenia.

Implementacja pamięci znajduje się w pliku \textit{replay\_memory.py}.

\subsubsection{Pozostałe ustawienia}

Wagi sieci aktualizowane są za pomocą metody RMSProp \textit{(ang. Root Mean Square Propagation)}. Prędkość uczenia \textit{(ang. learning rate)} została eksperymentalnie ustalona na 0.00025. Sieć jest uczona za pomocą paczek danych \textit{(ang. batch)} o wielkości 64.










