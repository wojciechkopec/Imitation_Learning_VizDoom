\chapter{Wstęp}

Kolejna rewolucja przemysłowa, której nadejście często upatrywane jest w gwałtownie rozprzestrzeniającym się użyciu narzędzi bazujących na uczeniu maszynowym, ma pociągnąć za sobą uwolnienie ludzkości od żmudnych i niewymagających kreatywności prac. Mimo że do prawdziwej, świadomej i twórczej sztucznej inteligencji jest ciągle daleko, to powszechne wykorzystanie komputerów do dalszej automatyzacji codziennych, powtarzalnych zadań wydaje się być w zasięgu ręki. Aby to osiągnąć, programy muszą być świadome otaczającego je środowiska i płynących z niego bodźców, adaptować swoje zachowanie do zmieniającej się sytuacji, zamiast stosować z góry ustalone reguły, oraz uczyć się nowych zadań bez potrzeby dokładnego opisu pożądanego zachowania. 

W 2012 roku autorzy pracy \cite{NIPS2012_4824} zrewolucjonizowali widzenie komputerowe. Stworzona przez nich głęboka sieć konwolucyjna osiągnęła drastyczny skok skuteczności w klasyfikacji obrazów wysokiej rozdzielczości do jednej z tysiąca ogólnych kategorii. Od tego czasu rozwiązania bazujące na ich architekturze wyrównały i przewyższyły skuteczność ludzi w tym samym zadaniu. W 2015 autorzy pracy \cite{mnih2015human} połączyli klasyczny algorytm \textit{uczenia ze wzmocnieniem}, \textit{Q-learning} z głębokimi sieciami neuronowymi, co pozwoliło stworzyć program, który na podstawie surowych obrazów 2D sam nauczył się grać w klasyczne gry Atari, dla wielu z nich osiągając wyniki znacznie przewyższające ludzkie. Te dwa odkrycia otworzyły drogę do programów uczących się działać w środowisku 3D, a dalej w realnym świecie, na podstawie surowej informacji obrazowej.

Wiodące metody uczenia ze wzmocnieniem, z \textit{Q-learningiem} na czele, mają jednak dwa poważne mankamenty.

Po pierwsze, dopiero po dłuższym czasie nauki programy zaczynają się zachowywać sensownie. Systemy, które po długim treningu zaczynają przewyższać człowieka, by jeszcze później zachwycić stosowaniem niespotykanych wcześniej strategii rozwiązywania znanych problemów, zaczynają naukę od długiego czasu poświęconego na losowe i bezsensowne działania oraz wielokrotne powtarzanie tych samych, nieskutecznych strategii. W zastosowaniach takich jak gry planszowe lub komputerowe, gdzie dostępne są dokładne symulatory rozgrywki, takie zachowanie podczas uczenia nie jest przeszkodą, ale dla realnych problemów, dla których nie ma możliwości symulacji albo symulacja jest zbyt wolna, takie zachowanie jest nie do zaakceptowania. Metoda, w której podczas nauki helikopter rozbija się w komputerowej symulacji tysiąc razy może być użyteczna. Metoda wymagająca rozbicia tysiąca prawdziwych helikopterów zdecydowanie nie.

Po drugie, wiodące metody uczenia ze wzmocnieniem są bardzo wolne. Nauka programów rozwiązujących aktualnie badane problemy trwa najczęściej dziesiątki godzin i wymaga sprzętu komputerowego o potężnych możliwościach obliczeniowych. Dodatkowo, jak wspomniano wcześniej, duża część tego czasu poświęcana jest na zrozumienie przez trenowane programy podstawowych zasad rządzących środowiskiem, w którym się znajdują, które dla człowieka są oczywiste.

Właśnie na wykorzystaniu wiedzy i prezentacji człowieka oparte są metody \textit{uczenia przez demonstrację}. W ramach działania tych metod ekspert, będący najczęściej człowiekiem potrafiącym skutecznie wykonywać dane zadanie (lub zewnętrzny, specjalizowany program komputerowy), demonstruje programowi jak się zachować, żeby osiągnąć oczekiwany rezultat. Program ma zadanie zbudować klasyfikator, który minimalizuje rozbieżności pomiędzy akcjami wykonywanymi w danej sytuacji przez program i przez eksperta.

Metody \textit{uczenia przez demonstrację} pozwalają na uzyskanie poprawnie zachowujących się w środowisku programów w czasie zdecydowanie krótszym niż w przypadku uczenia ze wzmocnieniem. W niektórych zastosowaniach konieczność dostępu do eksperta i brak umiejętności ,,kreatywnego'' generowania rozwiązań przez program może być przeszkodą, ale dla znacznej części problemów szybkie uzyskanie programu nieznacznie ustępującemu ludzkiemu ekspertowi jest w zupełności wystarczające. Co najważniejsze, dzięki \textit{uczeniu przez demonstrację} program potrafi wykonywać stawiane przed nim zadania bez konieczności wcześniejszego kontaktu ze środowiskiem, czyli bez konieczności rozbijania helikopterów. Opublikowana niedawno praca \cite{DBLP:journals/corr/HesterVPLSPSDOA17} pokazuje też, że \textit{uczenie przez demonstrację} można połączyć z \textit{Q-learningiem}, uzyskując metodę, która pozwala szybko uzyskać dobrze działający program nauczony na podstawie pokazów eksperta, który potrafi dalej samemu udoskonalać swoje działanie, osiągając umiejętności znacznie przewyższające przedstawione demonstracje.


Celem pracy jest zweryfikowanie, czy przy pomocy metod \textit{uczenia przez demonstrację} możliwe jest szybkie uzyskanie wyników porównywalnych z wynikami \textit {Q-learningu} w środowisku opartym na informacji obrazowej 3D.

W ramach pracy zostaną zaimplementowane na bazie głębokich sieci neuronowych metody \textit{kopiowania zachowań (ang. Behavioral Cloning)} i \textit{agregacji zbioru danych (ang. Dataset Aggregation, DAgger)}. Ich działanie będzie przeanalizowane pod względem wygody dla eksperta udzielającego demonstracji, a ich zachowanie będzie przetestowane na wybranych scenariuszach środowiska VizDoom i porównane z \textit {Q-learningiem} pod względem osiąganych wyników i czasu nauki.

Struktura pracy jest następująca. W rozdziale 2 opisano podstawy teoretyczne i podstawowe metody \textit{uczenia ze wzmocnieniem} i \textit{uczenia przez demonstrację}. W rozdziale 3 opisano środowisko VizDoom, w którym przeprowadzono wszystkie eksperymenty, oraz aktualne metody rozwiązywania problemów uczenia ze wzmocnieniem na podstawie informacji wizualnej. W rozdziale 4 opisano zaimplementowane w ramach pracy algorytmy. W rozdziale 5 opisano konfigurację i wyniki przeprowadzonych eksperymentów. Rozdział 6 stanowi podsumowanie pracy.




