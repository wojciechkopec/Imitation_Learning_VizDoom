\section{Proces decyzyjny Markowa}\label{mdp}

Środowisko, w którym porusza się program, albo inaczej agent, jest matematycznie zamodelowane jako proces decyzyjny Markowa \textit{(ang. Markov decision process, MDP)} \cite{bellman1954}. Oznacza to, że środowisko ma w każdym momencie czasu określony stan i umożliwia wykonanie określonych akcji, za które agent może otrzymać pozytywną lub negatywną nagrodę. Rezultatem wykonania akcji w danym stanie jest przejście do następnego stanu. Zakłada się, że nowy stan jest zależny tylko od stanu poprzedniego i wykonanej akcji. Takie środowisko ma właściwość braku pamięci (lub inaczej, \textit{własność Markowa}). Celem agenta jest zgromadzenie nagród o jak największej sumie wartości. Im bardziej odległe w przyszłości nagrody, tym mniej są wartościowe (są dyskontowane).

\vspace{5mm}

Formalnie, proces decyzyjny Markowa definiujemy jako piątkę $(S,A,T(\cdot,\cdot),R(\cdot,\cdot),\gamma)$, gdzie:
\begin{itemize}
\item $S$ jest skończonym zbiorem możliwych stanów środowiska,
\item $A_s$ jest skończonym zbiorem akcji możliwych w stanie $s$,
\item $T_a(s,s')$ - funkcja przejść, która reprezentuje prawdopodobieństwo trafienia do stanu $s'$ po wykonaniu akcji $a$ w stanie $s$,
\item $R_a(s,s')$ - funkcja nagrody, która określa nagrodę (lub wartością oczekiwaną nagrody, obie mogą być negatywne) otrzymywaną po wykonaniu akcji $a$ w stanie $s$ i trafieniu do stanu $s'$,
\item $\gamma \in [0,1]$ jest współczynnikiem dyskontowym, obniżającym wartość nagród uzyskanych w przyszłości.
\end{itemize}

Celem jest maksymalizacja zdyskontowanej sumy nagród $$\sum_{t=0}{\gamma^t R(s_t,s_{t+1})},$$

gdzie kolejne $t$ są kolejnymi momentami czasowymi. Ponadto:

\begin{itemize}
\item Polityką (strategią) $\pi$, realizowaną przez agenta, nazywamy funkcję $ \pi: S \rightarrow A$, która określa, jak agent powinien się zachować w danym stanie w celu osiągnięcia maksymalnej możliwej nagrody.
\item Funkcja użyteczności \textit{(ang. Utility)} $U(s)$ lub wartości \textit{(ang. Value)} $V(s)$ określa maksymalną oczekiwaną nagrodę, jaką agent może osiągnąć znajdując się stanie $s$ i postępując dalej zgodnie z aktualną polityką. Poniższe równanie oparte jest na równaniu Bellmana \cite{bellman1954}.

$$U(s) = V(s) = \max_{a \in A(s)} \sum_{s'} T_a(s,s')(R_a(s,s') + \gamma U(s'))$$
\item Funkcja $Q(s,a)$ określa maksymalną oczekiwaną nagrodę, jaką agent może osiągnąć wykonując w stanie $s$ akcję $a$ i postępując dalej zgodnie z aktualną polityką.

$$Q(s,a) = \sum_{s'} T_a(s,s')(R_a(s,s') + \gamma \max_{a' \in A(s)}Q(s',a'))$$

\end{itemize}

\vspace{5mm}

Znając funkcje przejść możliwe jest iteracyjne określenie optymalnej polityki działania agenta. 

W badanym problemie środowisko VizDoom jest \textit{częściowo obserwowalnym procesem decyzyjnym Markowa}, co oznacza, że stan obserwowany przez agenta nie zawiera pełnej informacji o środowisku. Środowisko to jest stochastyczne, co oznacza że skutki działań agenta nie są deterministyczne - wielokrotne wykonanie tej samej akcji w tym samym stanie może przynieść różne rezultaty.
