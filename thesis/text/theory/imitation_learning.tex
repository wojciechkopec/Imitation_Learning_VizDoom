\section{Uczenie przez demonstację}\label{imitation_learning}
Uczenie ze wzmocnieniem jest bardzo kosztowne obliczeniowo, a trudniejsze problemy wymagają przejścia setek tysięcy lub milionów klatek. Znaczną część tego czasu program spędza na początkowym poznawaniu możliwości i zasad rządzących środowiskiem, albo żmudnym, stopniowym poprawianiu suboptymalnych zachowań, skutkujących w końcu skuteczną polityką.

Ludzie i zwierzęta, których zachowanie często stanowi inspirację i motywację dla nowych rozwiązań algorytmicznych, potrafią odtwarzać czynności i zachowania na podstawie samej obserwacji wykonania danej czynności przez innych. Czerpiąc z tego przypadku, wskazane jest konstruowanie algorytmów, które będą potrafiły uczyć się zaawansowanych polityk nie na bazie milionów prób i błędów, ale na bazie obserwacji kilku lub jednego powtórzenia wykonania docelowego zadania. Taki cel stawiany jest przed uczeniem przez demonstrację.

W większości przypadków ekspert jest człowiekiem, który potrafi wykonać zadanie postawione przed agentem i potrafi sterować agentem w celu jego wykonania. Wykorzystanie ludzkiego eksperta pociąga za sobą poważne konsekwencje:
\begin{itemize}
\item \textbf{czas ludzkiego eksperta może być znacznie droższy niż czas maszynowy} - jeżeli dla badanego problemu dostępne jest wiarygodne i wydajne środowisko symulacyjne, to wykorzystanie klasycznego uczenia ze wzmocnieniem może być bardziej opłacalne niż zdobywanie prezentacji ludzkiego eksperta,
\item \textbf{zachowanie ludzkiego eksperta może być nieoptymalne} - sposób realizacji zadania przez ludzkiego eksperta może być suboptymalny. Uczenie ze wzmocnieniem wyprzedziło ludzkich ekspertów w wielu dziedzinach, na przykład w grze w szachy, backgammona i go. Opierając się na samym odtwarzaniu zachowań eksperta agent nie osiągnie lepszych niż on wyników.
\item \textbf{zachowanie ludzkiego eksperta może być niespójne} - realizacje tego samego zadania przez ludzkiego eksperta cechuje najczęściej pewna wariancja. Co więcej ekspert odpytywany o zachowania dla tego samego zadania podczas prezentacji w różnych warunkach może prezentować niespójne ze wcześniejszymi zachowania. W przypadku częsciowo obserwowalnych problemów decyzyjnych ekspert może mieć dostęp do większej ilości informacji niż agent, i podświadomie korzystać z nich podczas prezentacji.
\item  \textbf{konieczne jest zrozumienie "idei" za zachowaniami eksperta} - jak w każdym problemie uczenia, konieczna jest generalizacja. Przy uczeniu przez demonstację duży nacisk kładziony jest na użycie niewielkiej ilości danych uczących do zrozumienia skomplikowanych zadań, przez co odporność na przeuczenie i umiejętność generalizacji są wyjątkowo istotne.
\end{itemize}   

Omawiane poniżej publikacje wykorzystują często również ekspertów komputerowych - za ekspertów służą oddzielne, niezależne programy (przykładowo znające model środowiska i potrafiące dokonywać optymalnych decyzji). Komputerowi eksperci mogą służyć do ułatwienia i przyspieszenia badań nad algorytmami uczenia przez demonstrację, chociaż wykorzystanie eksperta komputerowego nie pociąga za sobą większości wymienionych wyżej problemów. Alternatywnie, eksperci komputerowi mogli by być uzasadnieni w sytuacji, w której jeden wielozadaniowy i uniersalny agent uczyłby się wykonywania wielu zadań na podstawie prezentacji wielu wyspecjalizowanych do danego zadania programów.

Celem uczenia przez demonstrację nie jest uzyskiwanie wyższych wyników niż uczenie ze wzmocnieniem, ale uzyskiwanie ich dużo szybciej.

\subsection{Kopiowanie zachowań}

Najprostszym podejściem do uczenia przez demonstrację, nazywanym kopiowaniem zachowań \textit{(ang. Behavioral cloning)} jest traktowanie go jak każdego innego problemu uczenia nadzorowanego. W kopiowaniu zachowań, w przeciwieństwie do maksymalizowania nagrody agenta w uczeniu ze wzmocnieniem, minimalizowana jest różnica pomiędzy pomiędzy polityką wyuczonego agenta a polityką eksperta.

To podejście zakłada jednak, jak każda metoda uczenia nadzorowanego, że dane uczące i testowe są niezależne i mają jednakowy rozkład, podczas gdy przy uczeniu przez demonstrację nauczona polityka ma bezpośredni wpływ na osiągane później stany, na podstawie których dana polityka będzie sprawdzana - intuicyjnie, trajektorie Eksperta będą przedstawiały dobre zachowania i będą odwiedzały tylko dobre stany leżące na ścieżce optymalnej polityki. Gdy klasyfikator popełni błąd w odwzorowywaniu polityki eksperta, agent najprawdopodobniej trafi do stanu nieodwiedzonego przez eksperta, w którym nie będzie wiedział jak się zachować, co z dużym prawdopodobieństwem oznacza popełnianie następnych błędów, ponieważ uczeń nie miał jak nauczyć się „podnoszenia się” po błędach.Jak dowiedziono w \cite{DBLP:journals/corr/abs-1011-0686} wynikający z tego błąd rośnie kwadratowo w stosunku do czasu trwania epizodów.[REF]

\subsection{Iteracja polityki}
Jednym ze sposobów radzenia sobie z tym problemem jest wprowadzanie małych zmian podczas iteracji polityki, dzięki czemu rozkład stanów dla nowej polityki jest bliski staremu, a agent może nauczyć się, jak ,,podnosić się'' po błędach. Algorytm opiera się na rozpoczynaniu od polityki całkowicie identycznej z polityką eksperta i stopniowym przechodzeniu na politykę wyuczoną. Aby to osiągnąć można wymagać, aby podczas uczenia uczeń mógł w każdej chwili zapytać eksperta, jaką akcję ekspert podjąłby w danym stanie. To rozwiązanie wymaga większej interakcji, ale może być zrealizowany dla wielu z praktycznych przykładów wykorzystania uczenia przez demonstrację.

\subsection{Uczenie w przód}
Pierwszym podejściem opisywanym przez \cite{DBLP:journals/corr/abs-1011-0686} jest uczenie w przód. Podejście opiera się na przeprowadzeniu kilku powtórzeń uczenia, gdzie w każdym kroku następuje uczenie się jednej polityki w jednym, konkretnym, momencie. Jeżeli uczenie będzie przeprowadzone po kolei dla każdego kolejnego kroku w czasie, to próbka uzyskanych stanów, na których prowadzone jest dalsze uczenie odpowiada dystrybucji stanów testowych, a algorytm może odpytać eksperta o właściwe działanie w osiągniętych stanach, dzięki czemu ekspert ma okazję zaprezentować jak ,,podnosić się'' po popełnieniu błędów przez klasyfikator. Powyższe podejście działa tylko dla zadań o skończonym horyzoncie czasowym, wymaga dużej interakcji z ekspertem i możliwości zrestartowania stanu środowiska i dokładnego odtworzenia uzyskanego wcześniej stanu, co w wielu przypadkach nie jest możliwe do zrealizowania.

\subsection{Iterowany probabilistyczny mieszający algorytm}\label{smile}
W celu wyeliminowania tych ograniczeń \cite{DBLP:journals/corr/abs-1011-0686} proponują Iterowany probabilistyczny mieszający algorytm. Opierając się na algorytmie iterowania polityki, algorytm w każdym kroku stosuje nową stochastyczną politykę wybierając z zadanym prawdopodobieństwem pomiędzy wykonywaniem polityki wyuczonej w poprzednim kroku i konstruowanej w danej iteracji nowej polityki. Prawdopodobieństwo wyboru nowej polityki jest niewielkie. Algorytm zaczyna od dokładnego wykonywania akcji eksperta. W każdej kolejnej iteracji prawdopodobieństwo odpytania eksperta jest coraz niższe i zbiega do 0.

Opisane rozwiązanie zostało z powodzeniem przetestowane na przykładzie grania w proste gry, gdzie danymi wejściowymi były surowe dane obrazowe z ekranu. Wadą tego podejścia jest brak odrzucania nieskutecznych polityk podczas iteracji, co może prowadzić do niestabilnych wyników.

Wykorzystanie analogicznego rozwiązania proponują \cite{DBLP:journals/corr/BengioVJS15}. Ich propozycja zakłada wybieranie z prawdopodobieństwem $e$ polityki eksperta i z prawdopodobieństwem $1-e$ polityki wyuczonej. Początkowa wartość $e$ powinna wynosić 1, aby klasyfikator mógł nauczyć się  odtwarzać politykę eksperta. Wraz z postępem nauki $e$ powinno stopniowo maleć do 0, aby klasyfikator miał szanse nauczyć się stanów nieodwiedzonych przez eksperta.
\subsection{Agregacją zbioru danych}
W kolejnej publikacji \cite{DBLP:journals/corr/abs-1011-0686} prezentują nowe podejście, nazwane Agregacją Zbioru Danych \textit{(ang. Dataset Aggregation, DAgger)}. W uproszczeniu, podejście to jest następujące: W pierwszej iteracji algorytm zbiera dane testowe stosując politykę pokazaną przez eksperta, po czym trenuje klasyfikator odwzorowujący zachowanie eksperta na danym zbiorze danych. W każdej kolejne iteracji algorytm stosuje politykę wygenerowaną w poprzedniej iteracji i dodaje dane uzyskane podczas jej stosowania do zbioru danych, po czym trenuje klasyfikator tak, by odwzorowywał zachowanie eksperta na całym zbiorze danych. Podobnie jak w poprzednim algorytmie, żeby przyspieszyć uczenie na pierwszych etapach algorytmu, dodano opcjonalną losową możliwość odpytania eksperta o decyzję dla wybranego stanu. Uzyskane z pomocą tej metody wyniki są wyraźnie lepsze od wyników z \ref{smile}.

\subsection{Podążanie za ekspertem a przewyższanie eksperta}
Dla wielu praktycznych problemów polityka eksperta może nie być optymalna. Algorytm, który stara się tylko i wyłącznie odwzorować politykę eksperta będzie generował w takiej sytuacji nieoptymalne wyniki, które w wielu praktycznych sytuacjach mogą znacznie odbiegać od optimum. Prostym rozwiązaniem tego problemu przedstawionym w \cite{DBLP:journals/corr/ChangKADL15} jest stosowanie e-zachłannej strategii – w każdym ruchu algorytm może wybrać z małym prawdopodobieństwem $e$ wykonanie losowej akcji zamiast akcji optymalnej według wyuczonej polityki. Dzięki temu algorytm może znaleźć lokalne optimum bliskie polityce eksperta. Warto zauważyć, że wymusza to posługiwanie się całościową nagrodą (kosztem) wykonania zadania jako celem optymalizacji, w przeciwieństwie do prostszego minimalizowania różnicy pomiędzy wynikami wyuczonej polityki a polityki eksperta.
