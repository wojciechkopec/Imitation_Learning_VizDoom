\section{Kopiowanie zachowań} \label{behavioral_cloning}

Kopiowanie zachowań (ang. \textit{Behavioral Cloning}) [REF] stanowi najprostsze podejście do uczenia z Ekspertem. Na podstawie zebranych trajektorii Eksperta uczony jest klasyfikator, który na mając na wejściu stan $s$ ma za zadanie przewidzieć, jaką akcję $a$ wykonałby w danej sytuacji Ekspert.

Dla wielu problemów Kopiowanie zachowań jest nieskuteczne (powody opisane są w [ROZDZIAŁ O TEORII]), ale na badanych scenariuszach VizDoom okazało się działać zaskakująco dobrze.

Kopiowanie zachowań jest podstawą bardziej zaawansowanych technik opisanych w dalszych punktach.

Ważną różnicę w stosunku do Q-learningu stanowi fakt, że czas trwania metody ogranicza się prawie wyłącznie do czasu zbierania trajektorii przez Eksperta. Czas trenowania klasyfikatora na zebranych danych powinien być pomijalny w stosunku do czasu zbierania. Oznacza to, że uczenie agenta za pomocą Kopiowania zachowań trwa znacznie krócej niż za pomocą np. Q-learningu, w którym agent musi przejść przez często przez wiele milionów klatek, by osiągnąć zadawalające wyniki. 

\subsection{Implementacja}

Kopiowanie zachowań sprowadza się do klasycznego problemu uczenia nadzorowanego z wieloma etykietami (możliwe akcje), z których tylko jedna jest poprawna. Dane wejściowe stanowią obrazy przedstawiające stan, wynikiem jest pasująca etykieta akcji. Jako klasyfikatora użyto głębokiej sieci neuronowej, o artchitekturze bazującej na architekturze z \ref{q_learning}.

Różnica pomiędzy architekturami sporowadza się do uczenia i interpretacji wyników. Przy Q-learningu sieć musi przewidywać wartość funkcji Q dla wszystkich akcji, a przy Kopiowaniu zachowań wystarczy znalezienie najbardziej pasującej akcji. Wykonane i przewidywane akcje są zakodowane za pomocą \textit{one-hot encoding}, a wynik uzyskiwany jest przez zastosowanie funkcji \textit{softmax} na wartościach q z architektury Q-learningu. Schemat sieci wygląda następująco.

[SCHEMAT SIECI]

Warto zwrócić uwagę, że dla większości scenariuszy problem uczenia stan $\to$ akcja ma niezbalansowany zbiór danych. Akcje ,,strzelaj'' występują znacznie rzadziej niż akcje ruchu. Oznacza to, że klasyfikator naiwnie nauczony na niezmienionym zbiorze danych mimo dobrej teoretycznej trafności będzie zupełnie nieskuteczny (np. nie wybierając nigdy akcji ,,strzelaj'').

Problem ten został rozwiązany przez zbalansowanie zbioru danych przy użyciu metody \textit{oversampling} [REF?] - dla każdej akcji przetrzymywany jest oddzielny zbiór danych, a użyte do uczenia próbki składają się w równych proporcjach z przykładów zastosowania każdej akcji. 

Warto zauważyć, że architektura Q-learningu wymaga, żeby każda możliwa akcja zdefiniowana była oddzielnie, łączenie z akcjami stanowiącymi złożenie innych akcji. Przykładowo akcje ,,lewo'', ,,prosto'' i ,,lewo i prosto'' są dla modelu zupełnie niezwiązane, mimo że często można byłoby stosować je zamiennie. W przypadku Kopiowania zachowań możliwe było by stworzenie klasyfikatora  stan $\to$ akcja, który jest jednocześnie klasyfikatorem binarnym dla każdej podstawowej akcji z osobna. Taki klasyfikator mógłby zamiast wybierać pomiędzy ,,lewo'', ,,prosto'' i ,,lewo i prosto'' zdecydować ,,lewo'' - tak i ,,prosto'' - tak, uzyskując ,,lewo i prosto''. Jednakże takie rozwiązanie nie zostało w tej pracy sprawdzone.

\subsection{Implementacja techniczna}

Zbieranie danych zostało zrealizowane za pomocą trybu SPECTATOR środowiska VizDoom, pozwalającemu agentowi obserować grę człowieka. Podczas gry eksperta zapisywane są stany, akcje i nagrody dla każdej kolejnej klatki. Trajektoria Eksperta serializowana jest do pliku za pomocą narzędzia \textit{pickle} dostępnego dla języka python.

Podczas gry Eksperta ustawiona jest rozdzielczość 640x480 pikseli, i takiej wielkości obrazy zapisywane są do pliku z trajektorią. Konsekwencją są bardzi duże rozmiary plików (3GB dla 6 tysięcy klatek). Obrazy nie są zmniejszane przed zapisem, żeby umożliwić swobodne manipulowanie wielkością obrazów używanych do uczenia klasyfikatora bez konieczności generowania nowych trajektorii Eksperta przy innych ustawieniach obrazu.

\vspace{5mm}

Tryb SPECTATOR ustawiany jest w następujący sposób.
\begin{lstlisting}[language=iPython]
game.set_window_visible(True)
game.set_mode(Mode.SPECTATOR)
game.set_screen_format(ScreenFormat.GRAY8)
\end{lstlisting}

Trajektoria Eksperta zbierana i zapisywana jest następująco.

\begin{lstlisting}[language=iPython]
    game.new_episode()
    while not game.is_episode_finished():
        state = game.get_state()
        game.advance_action()
        next_state = game.get_state()
        last_action = game.get_last_action()
        reward = game.get_last_reward()
        isterminal = game.is_episode_finished()

        print("State #" + str(state.number))
        print("Game variables: ", state.game_variables)
        print("Action:", last_action)
        print("Reward:", reward)
        print("=====================")
        memory.append((state.screen_buffer,last_action, next_state.screen_buffer, reward, isterminal))
\end{lstlisting}

Zapis trajektorii do pliku wygląda następująco.

\begin{lstlisting}[language=iPython]
with open('recorder_episode.pkl', 'wb') as f:
    pickle.dump(memory, f, 2)
\end{lstlisting}

\subsection{Zachowanie}
Eksperymenty były prowadzone na scenariuszach \nameref{scenario_hgs} i \nameref{scenario_dtc}.
W obu przypadkach Kopiowanie zachowań nauczone na już na podstawie 3 trajektorii Eksperta (6 tysięcy klatek) osiągało wizualnie sensowne zachowanie agentów i zaskakująco dobre wyniki.

Dla \nameref{scenario_dtc} agentowi zdarzało się strzelać w nieodpowiednim momencie lub niestrzelać w potrzebnym. Częste było też nieoptymalne zachowanie w postaci strzelania do odległych przeciwników, podczas gdy inni przeciwnicy mogli podkraść się za plecy agenta zabijając go i kończąc grę.

W tym scenariuszu zwiększanie liczby trajektorii Eksperta użytych do uczenia zwiększało wyniki agenta, który w dużej częsci gier osiągał wyniki bliskie maksymalnym i tylko sporadycznie dawał się na początku zajść od tyłu, co skutkowało pojedynczymi niskimi wynikami.

Dla \nameref{scenario_dtc} agentowi często zdarzało się blokować w rogach labiryntu, wpadając w nieskończoną pętlę akcji. Problem i rozwiązanie zostało opisane w \ref{presenting_expert}. Po wyelimininowaniu problemu agent zachowuje się wizualnie sensownie i osiąga przyzwoite wyniki. Problemem jest tylko nauczenie agenta omijania min. Na początku pracy ze scenariuszem ludzki Ekspert uznał miny za mniejsze apteczki i nie zauważył spadku życia po wejściu w nie. Wyniki uzyskiwane przez Eksperta wchodzącego czasami w miny były tylko nieznacznie lepsze od wyników agenta nauczonego na podstawie tych trajektorii.

W tym scenariuszu zwiększanie liczby trajektorii Eksperta użytych do uczenia nie zwiększało wyników agenta. 

\subsection{Wnioski}
W badanych scenariuszach Kopiowanie zachowań osiąga znacznie lepsze wyniki, niż sugerowałaby literatura i uzyskuje jest w ciągu ułamka czasu potrzebnego klasycznym metodom uczenia ze wzmocnieniem. Uzyskani agenci w większości przypadków zachowują się sensownie, chociaż czasem popełniają systematyczne błędy. Kopiowanie zachowań wydaje się świetnym punktem startowym dla VizDooma i wydaje się wskazane, żeby inne metody rozszerzały to podejście, zamiast je zastępować.
