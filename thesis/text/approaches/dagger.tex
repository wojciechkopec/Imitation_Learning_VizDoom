\section{DAgger}\label{dagger}

Podejście Agregacji Zbioru Danych (ang. Dataset Aggregation) \cite{DBLP:journals/corr/abs-1011-0686} zostało opisane we wcześniejszym rozdziale. Kluczowym założeniem metody jest odpytywanie eksperta o właściwe działanie w stanach, które nie były wcześniej przez niego pokazane (i nie należą do ,,poprawnych'' trajektorii), a które zostały odwiedzone przez agenta na skutek jego nieoptymalnego zachowania.

W rzeczywistości, dla bardziej skomplikowanych zadań, odpytywanie eksperta o decyzję dla każdego odwiedzonego przez agenta stanu jest niepraktyczne. Ocenianie wielu kolejnych stanów może być drogie i nużące dla eksperta, co może przekładać się na obniżoną jakość decyzji. Ocena dokonywana przez eksperta może też w praktyce różnić się w zależności od tego, czy ekspert napotkał dany stan podczas normalnego działania, czy podczas oceny pojednyczych, wyrwanych z kontekstu stanów.

Aby zminimalizować ten problem, konieczne jest określenie mniejszego podzbioru stanów, dla których potrzebna jest ocena eksperta.


\subsection{Implementacja} 
Zastosowana implementacja jest rozszerzeniem \ref{behavioral_cloning}. Pierwszym krokiem jest załadowane przygotowanych wcześniej trajektorii eksperta do pamięci agenta (zestawu danych).

Następnie agent rozpoczyna działanie, bazując na swoim aktualnym stanie wiedzy. Po wystąpieniu określonych warunków, definiujących potrzebę odpytania eksperta, działanie programu zostaje wstrzymane, a sterowanie przekazane jest do eksperta. Aby dostosować się do ograniczeń ludzkiego eksperta, po przekazaniu sterowania program przechodzi w tryb synchroniczny - przed każdą kolejną klatką czeka na reakcję eksperta. Ubocznym skutkiem tej implementacji jest pomijanie akcji ,,nic nie rób'', która jest wykonywana dopiero po wciśnięciu dedykowanego klawisza.

Po wystąpieniu określonych warunków, definiujących koniec potrzeby odpytywania eksperta, wszystkie stany i akcje odwiedzone w trakcie danej demonstacji dodawane są do pamięci agenta (trajektoria może być dodana do pamięci z większą wagą niż początkowe prezentacje - w przeciwnym wypadku dodanie nowych danych mogłoby nie być odczuwalne). Agent aktualizuje klasyfikator akcji na podstawie rozszerzonego zestawu danych, po czym przejmuje sterowanie od eksperta i wraca do normalnego działania bazując na uaktualnionym stanie wiedzy.

Po ponownym wystąpieniu określonych warunków, kontrola może ponownie zostać przekazana do eksperta.

\subsection{Przekazywanie sterowania}
Jednym z najważniejszych problemów jest zdefiniowanie, kiedy przekazywać sterowanie pomiędzy agentem a ekspertem. Wybór sposobu będzie decydował o tym, jak często ekspert będzie odpytywany i na ile istotna będzie uzyskana wiedza. Sprawdzone zostały trzy następujące sposoby.

\subsubsection{Losowe przekazanie sterowania}
\begin{enumerate}
\item Przed wykonaniem każdej akcji agent z bardzo małym prawdopodobieństwem może zdecydować o przekazaniu sterowania ekspertowi.
\item Po każdej akcji eksperta program z większym prawdopodbieństwem może zdecydować o przekazaniu sterowania do agenta.
\end{enumerate}

Losowe przekazywanie sterowania jest niepraktyczną metodą - dla analizowanych problemów agent nie potrzebuje pomocy eksperta przez większość czasu, więc losowo wybrane momenty przekazania sterowania w ogromnej większości nie dostarczają istotnej informacji. Zaletą jest natomiast automatyczność decyzji - program podczas gry agenta może działać w przyspieszonym tempie.

\subsubsection{Analiza niepewności sieci}
\begin{enumerate}
\item Przed wykonaniem każdej akcji sprawdzana jest niepewność sieci dla danego stanu, przybliżana przez 1 - wartość aktywacji funkcji softmax dla danej akcji. W przypadku wystąpienia zadanej liczby kolejnych niepewnych akcji sterowanie przekazywane jest do eksperta.
\item Po każdej akcji eksperta sprawdzana jest akcja, którą wykonałby agent. Jeżeli przez zadaną liczbę kolejnych kroków agent postąpiłby identycznie jak ekspert, to sterowanie wraca do agenta.
\end{enumerate}

Analiza niepewności sieci jest skuteczniejsza niż losowe przekazywanie sterowania. Wybrane tym sposobem okna działania eksperta częściej pokrywają się z oknami niepoprawnego działania agenta. W dalszym ciągu skuteczność metody nie jest zadowalająca - przyjęta miara niepewności powoduje, że agent może przekazać sterowanie do eskperta w obliczu sytuacji, dla której więcej niż jedna akcja jest sensowna. Porównywanie akcji agenta i eksperta przez zadaną liczbę kroków jest skuteczne dla problemów z niewielką liczbą akcji, ale nieskuteczne w sytuacji, w której podobny efekt można uzyskać za pomocą różnych sekwencji kroków (przykładowo dojście do danego punktu za pomocą permutacji akcji ,,lewo'', ,,prosto'' i ,,lewo i prosto''). Podobnie jak przy losowym podejściu, dzięki automatycznemu działaniu możliwe jest działanie programu w przyspieszonym tempie poczas gry agenta.

Przyjęta miara niepewności łączy niepewność wynikającą z niewiedzy sieci i niepewność wynikającą z równożędności akcji, podczas gdy konieczne jest analizowanie tylko pierwszej z nich. Z uwagi na zachowanie całej metody lepsza miara nie była analizowana.

\subsubsection{Decyzja eksperta}\label{expert_call}
\begin{enumerate}
\item ekspert obserwuje działanie agenta. ekspert przejmuje sterowanie kiedy uzna, że agent trafił do niepożądanego stanu.
\item Kiedy ekspert uzna, że agent nie jest już w niepożądanym stanie może oddać sterowanie agentowi.
\end{enumerate}

Decyzja eksperta jest najskuteczniejszą metodą i jest używana w dalszych eksperymentach. Ekspert może sam stwierdzić, kiedy działanie agenta jest niezgodne z pożądanym, maksymalizując skuteczność odpytywania eksperta. Oczywiście, ekspert musi spędzić więcej czasu obserwując działanie agenta, ale obserwacja jest dużo mniej uciążliwa (a zatem tańsza), niż prezentowanie. Problemem w niektórych sytacjach jest możliwość rozróżnienia, kiedy agent zachowa się niepożądanie i należałoby przejąć sterowanie - w wielu sytuacjach ekspert reaguje zbyt późno, żeby demonstracja była skuteczna.

\subsection{Techniczna implementacja}

Architektura sieci neuronowej jest identyczna z architekturą zastosowaną w \ref{behavioral_cloning}.

Implementacja przekazywania sterowania do eksperta w środowisku VizDoom byłaby wymagająca i czasochłonna, dlatego zastosowano znacznie prostsze, chociaż mniej eleganckie rozwiązanie.

Przy odpytywaniu eksperta o akcje program oczekuje na następny znak, który pojawi się na standardowym strumieniu wejścia programu (następny znak wpisany w konsoli). Wybrane znaki są przypisane do indeksów wybranych akcji, wpisanie nieznanego znaku powoduje wybranie akcji o indeksie 0, czyli ,,nic nie rób''.

\begin{lstlisting}[language=iPython]

    def get_expert_action(self):
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        try:
            tty.setraw(sys.stdin.fileno())
            move = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
        if move == 'j':
            return 4
        if move == 'l':
            return 2
        if move == 'a':
            return 1

        if move == 'i':
            return 1
        if move == 'u':
            return 5
        if move == 'o':
            return 3
        return 0
\end{lstlisting}

Przy metodzie decyzji eksperta konieczne jest asynchroniczne przetwarzanie działania eksperta. Program nie może oczekiwać na działanie eksperta, ale kiedy ekspert zarząda przekazania sterowania nastepna akcja powinna być już wykonywana przez niego.

W tym celu wykorzystano bibliotekę PyKeyboardEvent, która umożliwia reagowanie na systemowe informacje o wciśnięciu bądź puszczeniu klawiszy klawiatury. Poniższa klasa wywołuje zadaną funkcję po wciśnięciu lub puszczeniu zadanych klawiszy.


\begin{lstlisting}[language=iPython]
from __future__ import print_function
from pykeyboard import PyKeyboardEventwojciech_kopec_101675.pdf


class KeyMonitor(PyKeyboardEvent):
    def __init__(self, keys, keypress_handler):
        PyKeyboardEvent.__init__(self)
        self.keypress_handler = keypress_handler
        self.keys = set(keys)

    def tap(self, keycode, character, press):
        if character in self.keys:
            self.keypress_handler(character, press)
\end{lstlisting}

Wywoływana funkcja znajduje się poniżej. Klawisz 'p' przekazuje sterowanie pomiędzy ekspertem i agentem. Klawisze ',' i '.' zwalniają i przyspieszają działanie programu podczas gry agenta.

\begin{lstlisting}[language=iPython]
    def __toggle_user_input(self, character):
        if character == 'p':
            if self.expert_mode:
                self.learn_all()
            self.expert_mode = not self.expert_mode
            print ("Expert toggled: " + str(self.expert_mode))
        elif character == '.':
            self.framerate+=5
            print ("Framerate: " + str(self.framerate))
        elif character == ',':
            self.framerate -= 5
            print ("Framerate: " + str(self.framerate))
        return True
\end{lstlisting}

\subsection{Zachowanie}
Eksperymenty były prowadzone na przede wszystkim scenariuszu \nameref{scenario_hgs}. Początkowe trajektorie eksperta były wygenerowane zgodnie z opisem w rozdziale \ref{presenting_expert}.

Dla każdego z badanych scenariuszy uwzględnianie fragmentów trajektorii zaprezentowanych przez eksperta w trakcie gry obniża początkowo wyniki. Na skutek nauczenia się niespójnych zachowań eksperta agent zachowuje się mniej płynnie i częściej wpada w nieskończone pętle ruchów (przykładowo obracanie się naprzemian w lewo i w prawo w rogu labiryntu), co prowadzi do osiągania niższych wyników.

W scenariuszu \nameref{scenario_hgs} głównym problemem agenta opisanego w rodziale \nameref{behavioral_cloning} jest nieomijanie min i celem zastosowania podejścia DAgger jest wyeliminowanie tego problemu. Za każdym razem, kiedy agent zbliża się do min ekspert przejmuje kontrolę i omija miny bądź wybiera inną ścieżkę. Pary stan $\to$ akcja uzyskane w ten sposób są dodawane do pamięci dziesięciokrotnie.

Podczas pierwszych epizodów nauki wyniki uzyskiwane przez agenta zauważalnie się obniżają, a problem wchodzenia na miny nie jest wyeliminowany.

Następne epizody nauki powoli poprawiają wyniki agenta, przywracając je do poziomu wyjściowego lub nieznacznie go przewyższającego. Agent rzadziej wchodzi w miny, ale problem w dalszym ciągu pozostaje obecny.

Kolejne epizody nauki doprowadzają do przeuczenia - wyniki obniżąją się, a agent regularnie wpada w nieskończone pętle ruchów. Wchodzenie w miny nie zostaje wyelimininowane. Pogorszenie zachowania agenta może wynikać ze znużenia eksperta, a co za tym idzie zmiany jego zachowań i pogorszenia jego decyzji.

\subsection{Wnioski}
Dla wypróbowanych problemów DAgger nie wydaje się być skuteczny. W VizDoomie decyzje podejmowane przez ludzkiego eksperta są bardziej skomplikowane niż w Mario Cart, przedstawianym w publikacji, co, na skutek niespójności przedstawianych przez eksperta zachowań, zamiast do podwyższenia wyników agenta prowadzi do obniżania jego skuteczności. Zastosowane głębokie sieci neuronowe mogą też znacznie skuteczniej uogólniać wiedzę zdobytą podczas pierwszej prezentacji eksperta niż prostsze klasyfikatory SVM, a co za tym idzie nawet bez użycia DAggera agent potrafi znaleźć sensowne wyjście z większości sytuacji. Uzyskiwanie oceny eksperta jest uciążliwe i kosztowne.

